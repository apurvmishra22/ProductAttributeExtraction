{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVFh9kbmcFHq"
   },
   "source": [
    "**Title : Product attribute Extraction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VR7Qz1idTYY"
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iutG5s1scfT",
    "outputId": "21b1a874-8fe3-4dd5-f1e7-54f2f456619d"
   },
   "outputs": [],
   "source": [
    "#tensorflow_version 2.x\n",
    "!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgHNJY1lkA3b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPC99oTprrws"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofnZrDgYdtQT"
   },
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKPgDXd0kOZY"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNQxiGVsdwpj"
   },
   "source": [
    "Visualizinig the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5jSh-ZYFpedW",
    "outputId": "bd35ce04-cc9f-4350-fc0c-0cb3acb37aac"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G1FvOYod0Td"
   },
   "source": [
    "Taking the important features of the dataset for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FwqgmY7plAo"
   },
   "outputs": [],
   "source": [
    "dataset= dataset[['title','brand']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YwZqYJpd6hD"
   },
   "source": [
    "We will do the tokenization of the words persent in the dataset. Tokenizer will split the data which will then be used for mapping as a numeric vector by \n",
    "the help of glove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdqG9tuv3Dmo",
    "outputId": "1179f483-9c72-4ae2-dc0d-b3a4f27e5384"
   },
   "outputs": [],
   "source": [
    "tokenlist = []\n",
    "tokenlist.append(dataset['title'])\n",
    "tokenlist.append(dataset['brand'])\n",
    "tokenlist = np.ravel(tokenlist)\n",
    "print(tokenlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xxdx0AZmp-OF"
   },
   "outputs": [],
   "source": [
    "featurestoCare = 20000\n",
    "tokens = Tokenizer(num_words=featurestoCare)\n",
    "tokens.fit_on_texts(tokenlist)\n",
    "x = tokens.texts_to_sequences(dataset['title'].values)\n",
    "x = pad_sequences(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jofJVTjGuZGH"
   },
   "outputs": [],
   "source": [
    "y = dataset['brand'].values\n",
    "y = tokens.texts_to_sequences(y)\n",
    "y = pad_sequences(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOlHu557cuYn"
   },
   "source": [
    "Train-Test Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hgvKAlouOJh"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1,random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-bVXf4uc-pb"
   },
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLG8z03eurHy"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "# Glove Embedding Link: https://nlp.stanford.edu/projects/glove/\n",
    "f = open('glove.6B.300d.txt',encoding=\"utf8\") #Glove Embedding File \n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word_index.values())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9I3b6UxAvfiS",
    "outputId": "53c395b8-9387-4324-fbdb-7ba51db73f81"
   },
   "outputs": [],
   "source": [
    "word_index = tokens.word_index\n",
    "print('Found %s unique tokens.' % list(word_index.values())[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6LZBy7qvq0q"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_words = min(featurestoCare, list(word_index.values())[-1]) + 1\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    elif embedding_vector is None:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ym3_DCZADdGU"
   },
   "outputs": [],
   "source": [
    "T = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sJoSCawdR0q"
   },
   "source": [
    "Bi-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8tmswBN0WSp",
    "outputId": "344dedff-27a7-46c7-b63d-d9422c000786"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Embedding Layer \n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=T,\n",
    "                    trainable=True))\n",
    "\n",
    "# Bi_LSTM Layer\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True, go_backwards= True)))\n",
    "model.add((LSTM(100, go_backwards= True)))\n",
    "\n",
    "#Dense Layer\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "\n",
    "\n",
    "#Model Compilation\n",
    "model.compile(loss = 'hinge', optimizer='Adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odHFMAOgt6qH",
    "outputId": "b2f28a86-f327-44a0-ef99-f4acb4dbaf6d"
   },
   "outputs": [],
   "source": [
    "#Trainining Data\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXTHcpQm09D3",
    "outputId": "057d5259-098b-41a2-86f8-5147e27c183a"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEw4y6XiexL7"
   },
   "source": [
    "Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIk8gyMC0o3k",
    "outputId": "4e9838db-58b8-47d5-c5b7-6f65af78f8b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU0BBOqae0p9"
   },
   "source": [
    "Prediction in Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPH4_CctTK-I"
   },
   "outputs": [],
   "source": [
    "ypre = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Aauib0gTV1c",
    "outputId": "c4281c9d-b00a-4f04-9d11-f6dd9fcfa2ae"
   },
   "outputs": [],
   "source": [
    "print(ypre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fN6YzIkJ7d1z"
   },
   "outputs": [],
   "source": [
    "len = ypre.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLD-Ry6KfRYU"
   },
   "source": [
    "Converting the prediction from float to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2I8TJF4lxzZ"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "while i< len:\n",
    "    ypre[i][4] = round(ypre[i][4])\n",
    "    ypre[i][3] = round(ypre[i][3])\n",
    "    ypre[i][2] = round(ypre[i][2])\n",
    "    ypre[i][1] = round(ypre[i][1])\n",
    "    ypre[i][0] = round(ypre[i][0])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FHdcL_AfWEC"
   },
   "source": [
    "Vector to Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSMsCciUTbRR"
   },
   "outputs": [],
   "source": [
    "predicted = tokens.sequences_to_texts(ypre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at7mcKPCfaEX"
   },
   "source": [
    "Prediction in Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GP4lj0MGTbW",
    "outputId": "fe5f4d74-8e13-4998-b699-9c63464251cf"
   },
   "outputs": [],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EE769_Project_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
